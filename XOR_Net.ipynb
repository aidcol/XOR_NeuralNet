{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(2022)    # Set seed for reproducibility\n",
    "\n",
    "class XOR_Net:\n",
    "    # Training data\n",
    "    XOR_INPUTS = np.array([\n",
    "        [0, 0],\n",
    "        [1, 0],\n",
    "        [0, 1],\n",
    "        [1, 1]\n",
    "    ])\n",
    "\n",
    "    # Labels for training data\n",
    "    XOR_OUTPUTS = np.array([\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [0]\n",
    "    ])\n",
    "\n",
    "    NUM_EXAMPLES = XOR_INPUTS.shape[0]\n",
    "    NUM_INPUT_NEURONS = XOR_INPUTS.shape[1]       # 2\n",
    "    NUM_HIDDEN_NEURONS = 2\n",
    "    NUM_OUTPUT_NEURONS = XOR_OUTPUTS.shape[1]     # 1\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the network. Weights and biases are initialized randomly using Gaussian distribution (mean=0, variance=1)\n",
    "        \"\"\"\n",
    "        # Build a list containing the number of neurons in each layer\n",
    "        self.layers = [self.NUM_INPUT_NEURONS, self.NUM_HIDDEN_NEURONS, self.NUM_OUTPUT_NEURONS]\n",
    "        self.num_layers = len(self.layers)\n",
    "        \n",
    "        # Initialize lists of zero ndarrays to hold the bias vectors and weight matrices for each layer\n",
    "        # (set no biases for the first (input) layer)\n",
    "        self.biases = [np.zeros((y, 1)) for y in self.layers[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(self.layers[:-1], self.layers[1:])]\n",
    "    \n",
    "    def __str__(self):\n",
    "        network = \"\"\n",
    "        layers = str(self.layers)\n",
    "        network = \"Layers: \" + layers\n",
    "        return network\n",
    "    \n",
    "    def display_Parameters(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        for layer in range(len(self.weights)):\n",
    "            print(f\"Layer {layer}:\")\n",
    "            print(\"Weights:\")\n",
    "            print(self.weights)\n",
    "            print(\"\\nBiases:\")\n",
    "            print(self.biases)\n",
    "            print()\n",
    "\n",
    "    def feedforward(self, input):\n",
    "        \"\"\"\n",
    "        The feedforward behavior of the network.\n",
    "\n",
    "        Args:\n",
    "            - input: a (2x1) ndarray representing the binary input to the network\n",
    "        \n",
    "        Returns: a (1x1) ndarray representing the activation of the output neuron\n",
    "        \"\"\"\n",
    "        activation = input\n",
    "        for bias, Weight in zip(self.biases, self.weights):\n",
    "            activation = sigmoid(Weight @ activation + bias)\n",
    "        return activation\n",
    "\n",
    "    def predict(self, input):\n",
    "        \"\"\"\n",
    "        Uses the network to predict XOR.\n",
    "\n",
    "        Args:\n",
    "            - input: a (2x1) ndarray representing the binary input to the network\n",
    "        \n",
    "        Returns: a (1x1) ndarray representing a guess of the XOR output\n",
    "        \"\"\"\n",
    "        output = self.feedforward(input)\n",
    "        prediction = (output.squeeze() > 0.5) * 1   # guess '1' if output neuron is above 0.5 activation\n",
    "        return prediction\n",
    "    \n",
    "    def train(self, epochs, learn_rate):\n",
    "        \"\"\"\n",
    "        Train the network using gradient descent, and plot the cost vs. epochs.\n",
    "\n",
    "        Args:\n",
    "            - epochs: an integer, which is the number of training cycles\n",
    "            - learn_rate: a float, which is the step size for gradient descent\n",
    "        \"\"\"\n",
    "        costs = []\n",
    "        for _ in range(epochs):\n",
    "            del_b = [np.zeros(bias.shape) for bias in self.biases]\n",
    "            del_W = [np.zeros(Weight.shape) for Weight in self.weights]\n",
    "            training_costs = []\n",
    "\n",
    "            # Train on all XOR outputs\n",
    "            for input, output in zip(self.XOR_INPUTS, self.XOR_OUTPUTS):\n",
    "                input = input.reshape(2, 1)\n",
    "                delta_del_b, delta_del_W = self.backprop(input, output)\n",
    "                del_b = [db + ddb for db, ddb in zip(del_b, delta_del_b)]   # add change in gradient from one training example\n",
    "                del_W = [dW + ddW for dW, ddW in zip(del_W, delta_del_W)]   # add change in gradient from one training example\n",
    "                example_cost = cross_entropy_cost(self.feedforward(input), output).squeeze()\n",
    "                training_costs.append(example_cost)\n",
    "            \n",
    "            # Update the weights and biases\n",
    "            self.weights = [W - (learn_rate / self.NUM_EXAMPLES) * nW\n",
    "                            for W, nW in zip(self.weights, del_W)]\n",
    "            self.biases = [b - (learn_rate / self.NUM_EXAMPLES) * nb\n",
    "                            for b, nb in zip(self.biases, del_b)]\n",
    "            costs.append((1 / self.NUM_EXAMPLES) * sum(training_costs))\n",
    "\n",
    "        plt.plot(costs)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "    \n",
    "    def backprop(self, input, output):\n",
    "        \"\"\"\n",
    "        Compute gradient of the cost function.\n",
    "\n",
    "        Args:\n",
    "            - input: an (2x1) ndarray, which holds the binary inputs to the network\n",
    "            - output: an ndarray containing a single binary value (0, 1), which is the expected output of XOR on the input\n",
    "        \n",
    "        Returns a tuple '(del_b, del_W)' representing the gradient of the cost function.\n",
    "            - 'del_b' and 'del_W' are lists of numpy arrays, which are the bias vector and weight matrix for each\n",
    "              layer of the network\n",
    "        \"\"\"\n",
    "        del_b = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        del_W = [np.zeros(Weight.shape) for Weight in self.weights]\n",
    "\n",
    "        # Forward pass\n",
    "        activation = input\n",
    "        activations = [input]   # list to store activation vectors layer by layer\n",
    "        zs = [] # list to hold all z vectors, layer by layer (a = sigmoid(z))\n",
    "\n",
    "        for bias, Weight in zip(self.biases, self.weights):\n",
    "            z = Weight @ activation + bias\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # Backward pass\n",
    "        error = activations[-1] - output\n",
    "        del_b[-1] = error   \n",
    "        del_W[-1] = error @ activations[-2].transpose() \n",
    "        \n",
    "        for layer in range(2, self.num_layers):\n",
    "            z = zs[-layer]  # Iterate through the layers backwards\n",
    "            sigprime_z = sigmoid_prime(z)\n",
    "            error = (self.weights[-layer + 1].transpose() @ error) * sigprime_z\n",
    "            del_b[-layer] = error\n",
    "            del_W[-layer] = error @ activations[-layer - 1].transpose()\n",
    "        gradient = (del_b, del_W)\n",
    "\n",
    "        return gradient\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def cross_entropy_cost(a, y):\n",
    "        \"\"\"\n",
    "        The Cross-Entropy cost function.\n",
    "\n",
    "        Args *for this network*:\n",
    "            - y: a (1x1) ndarray, which is the expected XOR output\n",
    "            - a: a (1x1) ndarray, which contains the activation of the output neuron for one training example\n",
    "        \n",
    "        Returns the cross-entropy cost for the training example\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "Weights:\n",
      "[array([[-5.27899086e-04, -2.74901425e-01],\n",
      "       [-1.39285562e-01,  1.98468616e+00]]), array([[0.28210933, 0.76080866]])]\n",
      "\n",
      "Biases:\n",
      "[array([[0.],\n",
      "       [0.]]), array([[0.]])]\n",
      "\n",
      "Layer 1:\n",
      "Weights:\n",
      "[array([[-5.27899086e-04, -2.74901425e-01],\n",
      "       [-1.39285562e-01,  1.98468616e+00]]), array([[0.28210933, 0.76080866]])]\n",
      "\n",
      "Biases:\n",
      "[array([[0.],\n",
      "       [0.]]), array([[0.]])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = XOR_Net()\n",
    "\n",
    "# Show the parameters prior to training by gradient descent\n",
    "network.display_Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For [0 0], network output is: 0.6274888640591264\n",
      "Network predicts: 1\n",
      "For [1 0], network output is: 0.6212770288530577\n",
      "Network predicts: 1\n",
      "For [0 1], network output is: 0.6879763341643256\n",
      "Network predicts: 1\n",
      "For [1 1], network output is: 0.685416056582199\n",
      "Network predicts: 1\n"
     ]
    }
   ],
   "source": [
    "# Show the network's performance prior to training by gradient descent\n",
    "for input in network.XOR_INPUTS:\n",
    "    show_input = input\n",
    "    input = input.reshape(2,1)\n",
    "    output = network.feedforward(input).squeeze()\n",
    "    prediction = network.predict(input)\n",
    "    print(f\"For {show_input}, network output is: {output}\")\n",
    "    print(f\"Network predicts: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the network and display average cost per epoch\n",
    "network.train(5000, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "Weights:\n",
      "[array([[2.62490605, 2.62065491],\n",
      "       [5.63753757, 5.61251607]]), array([[-5.72680091,  6.24722183]])]\n",
      "\n",
      "Biases:\n",
      "[array([[-3.85811591],\n",
      "       [-1.88899609]]), array([[-3.0047804]])]\n",
      "\n",
      "Layer 1:\n",
      "Weights:\n",
      "[array([[2.62490605, 2.62065491],\n",
      "       [5.63753757, 5.61251607]]), array([[-5.72680091,  6.24722183]])]\n",
      "\n",
      "Biases:\n",
      "[array([[-3.85811591],\n",
      "       [-1.88899609]]), array([[-3.0047804]])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the parameters after training by gradient descent\n",
    "network.display_Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For [0 0], network output is: 0.09091341717765264\n",
      "Network predicts: 0\n",
      "For [1 0], network output is: 0.8589517240975013\n",
      "Network predicts: 1\n",
      "For [0 1], network output is: 0.8590355705515283\n",
      "Network predicts: 1\n",
      "For [1 1], network output is: 0.2074129559867437\n",
      "Network predicts: 0\n"
     ]
    }
   ],
   "source": [
    "# Show the network's performance after training by gradient descent\n",
    "for input in network.XOR_INPUTS:\n",
    "    show_input = input\n",
    "    input = input.reshape(2,1)\n",
    "    output = network.feedforward(input).squeeze()\n",
    "    prediction = network.predict(input)\n",
    "    print(f\"For {show_input}, network output is: {output}\")\n",
    "    print(f\"Network predicts: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0e52e45788a0b80e04c81cc4f919423dca9534a1b0aa541a9291bc3ae717e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
